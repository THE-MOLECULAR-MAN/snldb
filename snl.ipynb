{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from a public website and IMDb\n",
    "\n",
    "This notebook tracks my progress in scraping the website [snlarchives](http://www.snlarchives.net) for a Saturday Night Live dataset. I will also scrape the ratings for the episodes from IMDb. I chose to use the python library [scrapy](https://scrapy.org/) to do the work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy spiders\n",
    "\n",
    "Scrapy uses so called spiders to crawl the web. This means that I had to create a subclass from scrapy.Spider to begin. Spider *spider123* can be executed by using \n",
    "\n",
    "    scrapy runspider spider123.py \n",
    "\n",
    "in the python command line. This means that the code for each spider needs to be saved into a .py-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import re\n",
    "import string\n",
    "\n",
    "def removeTags(sXML):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  sText = re.sub(cleanr, '', sXML)\n",
    "  return sText\n",
    "\n",
    "class snl(scrapy.Spider):\n",
    "    name = 'snl'\n",
    "    start_urls = ['http://www.snlarchives.net/Seasons/']\n",
    "    base_url = \"http://www.snlarchives.net\"\n",
    "    base_url_imdb = \"http://www.imdb.com/title/tt0072562/episodes?season=\"\n",
    "\n",
    "    actor_seen = set()\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    printable = set(string.printable)\n",
    "\n",
    "    def removeTags(self, sXML):\n",
    "        sText = re.sub(self.cleanr, '', sXML)\n",
    "        return sText\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        snl = {}\n",
    "\n",
    "        # parsing snlarchives. Entrypoint is the seasons page at www.snlarchives.net/Seasons/\n",
    "        for season in response.css('div.thumbRectInner'):\n",
    "            sid = int(season.css('::text').extract_first())\n",
    "            year = 1974 + sid\n",
    "            next_page = '?{}'.format(year)\n",
    "\n",
    "            item_season = {}\n",
    "            item_season['sid'] = int(sid)\n",
    "            item_season['year'] = int(year)\n",
    "            item_season['type'] = 'season'\n",
    "\n",
    "            yield item_season\n",
    "\n",
    "            yield scrapy.Request(response.urljoin(next_page), callback=self.parseSeason, meta={'season': item_season})\n",
    "\n",
    "            # at this point we branch out to get the ratings from imdb.com \n",
    "            # the root URL for SNL is http://www.imdb.com/title/tt0072562\n",
    "            # we can find the episodes at http://www.imdb.com/title/tt0072562/episodes\n",
    "            # an we can select a certain episode like this http://www.imdb.com/title/tt0072562/episodes?season=1\n",
    "            yield scrapy.Request(self.base_url_imdb + str(sid), callback=self.parseRatingsSeason, meta={'season': item_season})\n",
    "\n",
    "            # remove statement to scrape more than one season\n",
    "            break\n",
    "\n",
    "    def parseRatingsSeason(self, response):\n",
    "        # parsing the ratings of the episodes of a season\n",
    "        item_season = response.meta['season']\n",
    "        eid = 0\n",
    "        for episode in response.css(\".eplist > .list_item > .image > a\"):\n",
    "            item_rating = {}\n",
    "            eid +=1\n",
    "            item_rating[\"type\"] = \"rating\"\n",
    "            item_rating[\"eid\"] = eid\n",
    "            item_rating[\"sid\"] = item_season[\"sid\"]\n",
    "            href_url = episode.css(\"a ::attr(href)\").extract_first()\n",
    "            url_split = href_url.split(\"?\")\n",
    "            href_url = \"http://www.imdb.com\" + url_split[0] + \"ratings\"\n",
    "            yield scrapy.Request(href_url, callback=self.parseRatingsEpisode, meta={'season': item_season, 'rating': item_rating})\n",
    "\n",
    "    def parseRatingsEpisode(self, response):\n",
    "        item_season = response.meta['season']\n",
    "        item_rating = response.meta['rating']\n",
    "        tables = response.css(\"table[cellpadding='0']\")\n",
    "        \n",
    "        # 1st table is vote distribution on rating\n",
    "        # 2nd table is vote distribution on age and gender\n",
    "        ratingTable = tables[0]\n",
    "        ageGenderTable = tables[1]\n",
    "\n",
    "        trCount = 0\n",
    "        for tableTr in ratingTable.css(\"tr\"):\n",
    "            if trCount > 0:\n",
    "                sRating = str(11 - trCount)\n",
    "                item_rating[sRating] = int(removeTags(tableTr.css(\"td\")[0].extract()))\n",
    "            trCount += 1\n",
    "        \n",
    "        trCount = 0\n",
    "        for tableTr in ageGenderTable.css(\"tr\"):\n",
    "            if trCount > 0:\n",
    "                tableTd = tableTr.css(\"td\").extract()\n",
    "                if len(tableTd) > 1:\n",
    "                    sKey = removeTags(tableTd[0]).lstrip().rstrip()\n",
    "                    sValue = int(removeTags(\"\".join(filter(lambda x: x in self.printable, tableTd[1]))))\n",
    "                    sValueAvg = float(removeTags(\"\".join(filter(lambda x: x in self.printable, tableTd[2]))))\n",
    "                    item_rating[sKey] = sValue\n",
    "                    item_rating[sKey + \"_avg\"] = sValueAvg\n",
    "            trCount+=1\n",
    "        yield item_rating\n",
    "\n",
    "    def parseSeason(self, response):\n",
    "        # parsing a season (e.g. www.snlarchives.net/Seasons/?1975)\n",
    "        # episodes is already chosen\n",
    "        item_season = response.meta['season']\n",
    "\n",
    "        for episode in response.css('a'):\n",
    "            href_url = episode.css(\"a ::attr(href)\").extract_first()\n",
    "            if href_url.startswith(\"/Episodes/?\") and len(href_url)==19: \n",
    "                episode_url = self.base_url + href_url\n",
    "                yield scrapy.Request(episode_url, callback=self.parseEpisode, meta={'season': item_season})\n",
    "                # remove statement to scrape more than one episode\n",
    "                break\n",
    "\n",
    "    def parseEpisode(self, response):\n",
    "        item_season = response.meta['season']\n",
    "\n",
    "        episode = {}\n",
    "        episode['sid'] = item_season['sid']\n",
    "        episode['year'] = item_season['year']\n",
    "        episode['type'] = 'episode'\n",
    "\n",
    "        for epInfoTr in response.css(\"table.epGuests tr\"):\n",
    "            epInfoTd = epInfoTr.css(\"td\")\n",
    "            if epInfoTd[0].css(\"td p ::text\").extract_first() == 'Aired:':\n",
    "                airedInfo = epInfoTd[1].css(\"td p ::text\").extract()\n",
    "                episode['aired'] = airedInfo[0][:-2]\n",
    "                episode['eid'] = int(airedInfo[2].split(' ')[0][1:])\n",
    "                break\n",
    "\n",
    "        yield episode\n",
    "        # initially the titles tab is opened\n",
    "        for sketchInfo in response.css(\"div.sketchWrapper\"):\n",
    "            sketch = {}\n",
    "            href_url = sketchInfo.css(\"a ::attr(href)\").extract_first()\n",
    "            sketch['sid'] = item_season['sid']\n",
    "            sketch['eid'] = episode['eid']\n",
    "            sketch['tid'] = int(href_url.split('?')[1])\n",
    "            sketch['title'] = sketchInfo.css(\".title ::text\").extract_first()\n",
    "            sketch['type'] = 'title'\n",
    "            sketch['titleType'] = sketchInfo.css(\".type ::text\").extract_first()\n",
    "            if sketch['title'] == None:\n",
    "                sketch['title'] = \"\"\n",
    "\n",
    "            sketch_url = self.base_url + href_url\n",
    "            yield scrapy.Request(sketch_url, callback=self.parseTitle, meta={'title': sketch})\n",
    "            # remove statement to scrape more than one sketch\n",
    "            break\n",
    "\n",
    "    def parseTitle(self, response):\n",
    "        sketch = response.meta['title']\n",
    "        for actor in response.css(\".roleTable > tr\"):\n",
    "            actor_dict = {}\n",
    "            actor_sketch = {}\n",
    "            actor_dict['name'] = actor.css(\"td ::text\").extract_first()\n",
    "            if actor_dict['name'] != None:\n",
    "                actor_dict['type'] = 'actor'\n",
    "                href_url = actor.css(\"td > a ::attr(href)\").extract_first()\n",
    "                if href_url != None:\n",
    "                    if href_url.split('?')[0] == '/Cast/':\n",
    "                        actor_dict['aid'] = href_url.split('?')[1]\n",
    "                        actor_dict['isCast'] = 1\n",
    "                        actor_sketch['actorType'] = 'cast'\n",
    "                    elif href_url.split('?')[0] == '/Crew/':\n",
    "                        actor_dict['aid'] = href_url.split('?')[1]\n",
    "                        actor_dict['isCast'] = 0\n",
    "                        actor_sketch['actorType'] = 'crew'\n",
    "\n",
    "                else:\n",
    "                    actor_dict['aid'] = actor_dict['name']\n",
    "                    actor_dict['isCast'] = 0\n",
    "                    actor_sketch['actorType'] = actor.css(\"td ::attr(class)\").extract_first()\n",
    "                    if actor_sketch['actorType'] == None:\n",
    "                        actor_sketch['actorType'] = \"unknown\"\n",
    "                \n",
    "                if not actor_dict['aid'] in self.actor_seen:\n",
    "                    self.actor_seen.add(actor_dict['aid'])\n",
    "                    yield actor_dict\n",
    "\n",
    "                actor_sketch['tid'] = sketch['tid']\n",
    "                actor_sketch['sid'] = sketch['sid']\n",
    "                actor_sketch['eid'] = sketch['eid']\n",
    "                actor_sketch['aid'] = actor_dict['aid']\n",
    "                actor_sketch['type'] = 'actor_sketch'\n",
    "                yield actor_sketch\n",
    "        yield sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the spider\n",
    "You can also run a spider from a script. To do this you need to import the additional class *CrawlerProcess*. Note that scraping can take a lot of time, so I inserted *break*-statements into *snl_spider*. The code above will only scrape one sketch of one episode of one season. This will be done in a reasonable time. If you want to scrape everything you should call your spider from the commandline using the following command:\n",
    "\n",
    "    scrapy runspider ./spider/snl.py -o ./data/snl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 09:08:53 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-02-08 09:08:53 [scrapy] INFO: Overridden settings: {'FEED_URI': 'snl.json', 'FEED_FORMAT': 'json', 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'snl.json'\n",
    "})\n",
    "\n",
    "#process.crawl(snl_spider)\n",
    "#process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data\n",
    "Once the spider is finished you have a .json-file that contains all of your data. Let's read it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_df = pd.read_json(\"./data/snl_v9.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1', '10', '2', '3', '4', '5', '6', '7', '8', '9', 'Aged 18-29',\n",
       "       'Aged 18-29_avg', 'Aged 30-44', 'Aged 30-44_avg', 'Aged 45+',\n",
       "       'Aged 45+_avg', 'Aged under 18', 'Aged under 18_avg', 'Females',\n",
       "       'Females Aged 18-29', 'Females Aged 18-29_avg', 'Females Aged 30-44',\n",
       "       'Females Aged 30-44_avg', 'Females Aged 45+', 'Females Aged 45+_avg',\n",
       "       'Females under 18', 'Females under 18_avg', 'Females_avg', 'IMDb staff',\n",
       "       'IMDb staff_avg', 'IMDb users', 'IMDb users_avg', 'Males',\n",
       "       'Males Aged 18-29', 'Males Aged 18-29_avg', 'Males Aged 30-44',\n",
       "       'Males Aged 30-44_avg', 'Males Aged 45+', 'Males Aged 45+_avg',\n",
       "       'Males under 18', 'Males under 18_avg', 'Males_avg', 'Non-US users',\n",
       "       'Non-US users_avg', 'Top 1000 voters', 'Top 1000 voters_avg',\n",
       "       'US users', 'US users_avg', 'actorType', 'aid', 'aired', 'eid',\n",
       "       'isCast', 'name', 'sid', 'tid', 'title', 'titleType', 'type', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data frame contains every column from every type of data. This means that the first thing to do is split the data into separate dataframes. To do this I inserted the column *type* to every entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_season = snl_df[snl_df.type == 'season'][['sid','year']].reset_index()\n",
    "snl_episode = snl_df[snl_df.type == 'episode'][['sid','eid','year','aired']].reset_index()\n",
    "snl_title = snl_df[snl_df.type == 'title'][['sid','eid','tid','title','titleType']].reset_index()\n",
    "snl_actor = snl_df[snl_df.type == 'actor'][['aid','name','isCast']].reset_index()\n",
    "snl_actor_sketch = snl_df[snl_df.type == 'actor_sketch'][['sid','eid','tid','aid','actorType']].reset_index()\n",
    "snl_rating = snl_df[snl_df.type == 'rating'][['sid','eid','1', '10', '2', '3', '4', '5', '6', '7', '8', '9', 'Aged 18-29',\n",
    "       'Aged 18-29_avg', 'Aged 30-44', 'Aged 30-44_avg', 'Aged 45+',\n",
    "       'Aged 45+_avg', 'Aged under 18', 'Aged under 18_avg', 'Females',\n",
    "       'Females Aged 18-29', 'Females Aged 18-29_avg', 'Females Aged 30-44',\n",
    "       'Females Aged 30-44_avg', 'Females Aged 45+', 'Females Aged 45+_avg',\n",
    "       'Females under 18', 'Females under 18_avg', 'Females_avg', 'IMDb staff',\n",
    "       'IMDb staff_avg', 'IMDb users', 'IMDb users_avg', 'Males',\n",
    "       'Males Aged 18-29', 'Males Aged 18-29_avg', 'Males Aged 30-44',\n",
    "       'Males Aged 30-44_avg', 'Males Aged 45+', 'Males Aged 45+_avg',\n",
    "       'Males under 18', 'Males under 18_avg', 'Males_avg', 'Non-US users',\n",
    "       'Non-US users_avg', 'Top 1000 voters', 'Top 1000 voters_avg',\n",
    "       'US users', 'US users_avg']].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix some datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_rating['sid'] = snl_rating['sid'].astype(int)\n",
    "snl_rating['eid'] = snl_rating['eid'].astype(int)\n",
    "snl_rating['1'] = snl_rating['1'].fillna(0).astype(int)\n",
    "snl_rating['2'] = snl_rating['2'].fillna(0).astype(int)\n",
    "snl_rating['3'] = snl_rating['3'].fillna(0).astype(int)\n",
    "snl_rating['4'] = snl_rating['4'].fillna(0).astype(int)\n",
    "snl_rating['5'] = snl_rating['5'].fillna(0).astype(int)\n",
    "snl_rating['6'] = snl_rating['6'].fillna(0).astype(int)\n",
    "snl_rating['7'] = snl_rating['7'].fillna(0).astype(int)\n",
    "snl_rating['8'] = snl_rating['8'].fillna(0).astype(int)\n",
    "snl_rating['9'] = snl_rating['9'].fillna(0).astype(int)\n",
    "snl_rating['10'] = snl_rating['10'].fillna(0).astype(int)\n",
    "snl_rating['Aged under 18'] = snl_rating['Aged under 18'].fillna(0).astype(int)\n",
    "snl_rating['Aged 18-29'] = snl_rating['Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Aged 30-44'] = snl_rating['Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Aged 45+'] = snl_rating['Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['Females under 18'] = snl_rating['Females under 18'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 18-29'] = snl_rating['Females Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 30-44'] = snl_rating['Females Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 45+'] = snl_rating['Females Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['Males under 18'] = snl_rating['Males under 18'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 18-29'] = snl_rating['Males Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 30-44'] = snl_rating['Males Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 45+'] = snl_rating['Males Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['IMDb staff'] = snl_rating['IMDb staff'].fillna(0).astype(int)\n",
    "snl_rating['IMDb users'] = snl_rating['IMDb users'].fillna(0).astype(int)\n",
    "snl_rating['US users'] = snl_rating['US users'].fillna(0).astype(int)\n",
    "snl_rating['Non-US users'] = snl_rating['Non-US users'].fillna(0).astype(int)\n",
    "snl_rating['Top 1000 voters'] = snl_rating['Top 1000 voters'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_episode = snl_episode[np.isfinite(snl_episode['eid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_season.sid = pd.to_numeric(snl_season.sid)\n",
    "snl_season.year = pd.to_numeric(snl_season.year)\n",
    "\n",
    "snl_episode.sid = pd.to_numeric(snl_episode.sid)\n",
    "snl_episode.eid = pd.to_numeric(snl_episode.eid)\n",
    "snl_episode.year = pd.to_numeric(snl_episode.year)\n",
    "\n",
    "snl_title.sid = pd.to_numeric(snl_title.sid)\n",
    "snl_title.eid = pd.to_numeric(snl_title.eid)\n",
    "snl_title.tid = pd.to_numeric(snl_title.tid)\n",
    "\n",
    "snl_actor.isCast = pd.to_numeric(snl_actor.isCast)\n",
    "\n",
    "snl_actor_sketch.sid = pd.to_numeric(snl_actor_sketch.sid)\n",
    "snl_actor_sketch.eid = pd.to_numeric(snl_actor_sketch.eid)\n",
    "snl_actor_sketch.tid = pd.to_numeric(snl_actor_sketch.tid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 sketches in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sid</th>\n",
       "      <th>eid</th>\n",
       "      <th>tid</th>\n",
       "      <th>title</th>\n",
       "      <th>titleType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>197701221.0</td>\n",
       "      <td>Injured John</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>197805201.0</td>\n",
       "      <td>Nixon's Book</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>197805131.0</td>\n",
       "      <td>Paraquat</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>198101241.0</td>\n",
       "      <td>America Not Held Hostage Anymore</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>198104111.0</td>\n",
       "      <td>Storage Room</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>121</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>198202271.0</td>\n",
       "      <td>CBS Evening News</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>135</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>197905261.0</td>\n",
       "      <td>Mr. Bill</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>151</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>197804221.0</td>\n",
       "      <td>Rock Concert</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>153</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>198205151.0</td>\n",
       "      <td>Kaufman Fight</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>158</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>198205221.0</td>\n",
       "      <td>Women's Room</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  sid   eid          tid                             title  \\\n",
       "0     82  2.0  12.0  197701221.0                      Injured John   \n",
       "1     91  3.0  20.0  197805201.0                      Nixon's Book   \n",
       "2     99  3.0  19.0  197805131.0                          Paraquat   \n",
       "3    113  6.0   8.0  198101241.0  America Not Held Hostage Anymore   \n",
       "4    118  6.0  13.0  198104111.0                      Storage Room   \n",
       "5    121  7.0  13.0  198202271.0                  CBS Evening News   \n",
       "6    135  4.0  20.0  197905261.0                          Mr. Bill   \n",
       "7    151  3.0  18.0  197804221.0                      Rock Concert   \n",
       "8    153  7.0  19.0  198205151.0                     Kaufman Fight   \n",
       "9    158  7.0  20.0  198205221.0                      Women's Room   \n",
       "\n",
       "      titleType  \n",
       "0  Cold Opening  \n",
       "1  Cold Opening  \n",
       "2  Cold Opening  \n",
       "3  Cold Opening  \n",
       "4  Cold Opening  \n",
       "5  Cold Opening  \n",
       "6  Cold Opening  \n",
       "7  Cold Opening  \n",
       "8  Cold Opening  \n",
       "9  Cold Opening  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl_title.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's store the files on the disc so that we can share them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_season.to_csv(\"./db/snl_season.csv\")\n",
    "snl_episode.to_csv(\"./db/snl_episode.csv\")\n",
    "snl_title.to_csv(\"./db/snl_title.csv\")\n",
    "snl_actor.to_csv(\"./db/snl_actor.csv\")\n",
    "snl_actor_sketch.to_csv(\"./db/snl_actor_sketch.csv\")\n",
    "snl_rating.to_csv(\"./db/snl_ratings.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
