{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from a public website and IMDb\n",
    "\n",
    "This notebook tracks my progress in scraping the website [snlarchives](http://www.snlarchives.net) for a Saturday Night Live dataset. I will also scrape the ratings for the episodes from IMDb. I chose to use the python library [scrapy](https://scrapy.org/) to do the work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy spiders\n",
    "\n",
    "Scrapy uses so called spiders to crawl the web. This means that I had to create a subclass from scrapy.Spider to begin. Spider *spider123* can be executed by using \n",
    "\n",
    "    scrapy runspider spider123.py \n",
    "\n",
    "in the python command line. This means that the code for each spider needs to be saved into a .py-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import re\n",
    "import string\n",
    "\n",
    "def removeTags(sXML):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  sText = re.sub(cleanr, '', sXML)\n",
    "  return sText\n",
    "\n",
    "class snl(scrapy.Spider):\n",
    "    name = 'snl'\n",
    "    # http://www.snlarchives.net/\n",
    "    start_urls = ['http://www.snlarchives.net/Seasons/']\n",
    "    base_url = \"http://www.snlarchives.net\"\n",
    "    base_url_imdb = \"http://www.imdb.com/title/tt0072562/episodes?season=\"\n",
    "\n",
    "    actor_seen = set()\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    printable = set(string.printable)\n",
    "\n",
    "    def removeTags(self, sXML):\n",
    "        sText = re.sub(self.cleanr, '', sXML)\n",
    "        return sText\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        snl = {}\n",
    "\n",
    "        # parsing snlarchives. Entrypoint is the seasons page at www.snlarchives.net/Seasons/\n",
    "        print(response)\n",
    "        for season in response.css('div.mt-2 container-fluid'):\n",
    "            sid = int(season.css('::text').extract_first())\n",
    "            year = 1974 + sid\n",
    "            next_page = '?{}'.format(year)\n",
    "\n",
    "            item_season = {}\n",
    "            item_season['sid'] = int(sid)\n",
    "            item_season['year'] = int(year)\n",
    "            item_season['type'] = 'season'\n",
    "\n",
    "            yield item_season\n",
    "\n",
    "            yield scrapy.Request(response.urljoin(next_page), callback=self.parseSeason, meta={'season': item_season})\n",
    "\n",
    "            # at this point we branch out to get the ratings from imdb.com \n",
    "            # the root URL for SNL is http://www.imdb.com/title/tt0072562\n",
    "            # we can find the episodes at http://www.imdb.com/title/tt0072562/episodes\n",
    "            # an we can select a certain episode like this http://www.imdb.com/title/tt0072562/episodes?season=1\n",
    "            yield scrapy.Request(self.base_url_imdb + str(sid), callback=self.parseRatingsSeason, meta={'season': item_season})\n",
    "\n",
    "\n",
    "    def parseRatingsSeason(self, response):\n",
    "        # parsing the ratings of the episodes of a season\n",
    "        item_season = response.meta['season']\n",
    "        eid = 0\n",
    "        for episode in response.css(\".eplist > .list_item > .image > a\"):\n",
    "            item_rating = {}\n",
    "            eid +=1\n",
    "            item_rating[\"type\"] = \"rating\"\n",
    "            item_rating[\"eid\"] = eid\n",
    "            item_rating[\"sid\"] = item_season[\"sid\"]\n",
    "            href_url = episode.css(\"a ::attr(href)\").extract_first()\n",
    "            url_split = href_url.split(\"?\")\n",
    "            href_url = \"http://www.imdb.com\" + url_split[0] + \"ratings\"\n",
    "            yield scrapy.Request(href_url, callback=self.parseRatingsEpisode, meta={'season': item_season, 'rating': item_rating})\n",
    "\n",
    "    def parseRatingsEpisode(self, response):\n",
    "        item_season = response.meta['season']\n",
    "        item_rating = response.meta['rating']\n",
    "        tables = response.css(\"table[cellpadding='0']\")\n",
    "        \n",
    "        # 1st table is vote distribution on rating\n",
    "        # 2nd table is vote distribution on age and gender\n",
    "        ratingTable = tables[0]\n",
    "        ageGenderTable = tables[1]\n",
    "\n",
    "        trCount = 0\n",
    "        for tableTr in ratingTable.css(\"tr\"):\n",
    "            if trCount > 0:\n",
    "                sRating = str(11 - trCount)\n",
    "                item_rating[sRating] = int(removeTags(tableTr.css(\"td\")[0].extract()))\n",
    "            trCount += 1\n",
    "        \n",
    "        trCount = 0\n",
    "        for tableTr in ageGenderTable.css(\"tr\"):\n",
    "            if trCount > 0:\n",
    "                tableTd = tableTr.css(\"td\").extract()\n",
    "                if len(tableTd) > 1:\n",
    "                    sKey = removeTags(tableTd[0]).lstrip().rstrip()\n",
    "                    sValue = int(removeTags(\"\".join(filter(lambda x: x in self.printable, tableTd[1]))))\n",
    "                    sValueAvg = float(removeTags(\"\".join(filter(lambda x: x in self.printable, tableTd[2]))))\n",
    "                    item_rating[sKey] = sValue\n",
    "                    item_rating[sKey + \"_avg\"] = sValueAvg\n",
    "            trCount+=1\n",
    "        yield item_rating\n",
    "\n",
    "    def parseSeason(self, response):\n",
    "        # parsing a season (e.g. www.snlarchives.net/Seasons/?1975)\n",
    "        # episodes is already chosen\n",
    "        item_season = response.meta['season']\n",
    "\n",
    "        for episode in response.css('a'):\n",
    "            href_url = episode.css(\"a ::attr(href)\").extract_first()\n",
    "            if href_url.startswith(\"/Episodes/?\") and len(href_url)==19: \n",
    "                episode_url = self.base_url + href_url\n",
    "                yield scrapy.Request(episode_url, callback=self.parseEpisode, meta={'season': item_season})\n",
    "                # remove statement to scrape more than one episode\n",
    "                #break\n",
    "\n",
    "    def parseEpisode(self, response):\n",
    "        item_season = response.meta['season']\n",
    "\n",
    "        episode = {}\n",
    "        episode['sid'] = item_season['sid']\n",
    "        episode['year'] = item_season['year']\n",
    "        episode['type'] = 'episode'\n",
    "\n",
    "        for epInfoTr in response.css(\"table.epGuests tr\"):\n",
    "            epInfoTd = epInfoTr.css(\"td\")\n",
    "            if epInfoTd[0].css(\"td p ::text\").extract_first() == 'Aired:':\n",
    "                airedInfo = epInfoTd[1].css(\"td p ::text\").extract()\n",
    "                episode['aired'] = airedInfo[0][:-2]\n",
    "                episode['eid'] = int(airedInfo[2].split(' ')[0][1:])\n",
    "            if epInfoTd[0].css(\"td p ::text\").extract_first() == 'Host:':\n",
    "                host = epInfoTd[1].css(\"td p ::text\").extract()\n",
    "                episode['host'] = host[0]\n",
    "            if epInfoTd[0].css(\"td p ::text\").extract_first() == 'Hosts:':\n",
    "                host = epInfoTd[1].css(\"td p ::text\").extract()\n",
    "                episode['host'] = host\n",
    "\n",
    "        yield episode\n",
    "        # initially the titles tab is opened\n",
    "        for sketchInfo in response.css(\"div.sketchWrapper\"):\n",
    "            sketch = {}\n",
    "            href_url = sketchInfo.css(\"a ::attr(href)\").extract_first()\n",
    "            sketch['sid'] = item_season['sid']\n",
    "            sketch['eid'] = episode['eid']\n",
    "            sketch['tid'] = int(href_url.split('?')[1])\n",
    "            sketch['title'] = sketchInfo.css(\".title ::text\").extract_first()\n",
    "            sketch['type'] = 'title'\n",
    "            sketch['titleType'] = sketchInfo.css(\".type ::text\").extract_first()\n",
    "            if sketch['title'] == None:\n",
    "                sketch['title'] = \"\"\n",
    "\n",
    "            sketch_url = self.base_url + href_url\n",
    "            yield scrapy.Request(sketch_url, callback=self.parseTitle, meta={'title': sketch, 'episode': episode})\n",
    "            # remove statement to scrape more than one sketch\n",
    "            #break\n",
    "\n",
    "    def parseTitle(self, response):\n",
    "        sketch = response.meta['title']\n",
    "        episode = response.meta['episode']\n",
    "        actor_seen_title = set()\n",
    "        for actor in response.css(\".roleTable > tr\"):\n",
    "            actor_dict = {}\n",
    "            actor_sketch = {}\n",
    "            actor_dict['name'] = actor.css(\"td ::text\").extract_first()\n",
    "            if actor_dict['name'] == ' ... ':\n",
    "                actor_dict['name'] = episode['host']\n",
    "            if actor_dict['name'] != None:\n",
    "                actor_dict['type'] = 'actor'\n",
    "                href_url = actor.css(\"td > a ::attr(href)\").extract_first()\n",
    "                if href_url != None:\n",
    "                    if href_url.split('?')[0] == '/Cast/':\n",
    "                        actor_dict['aid'] = href_url.split('?')[1]\n",
    "                        actor_dict['isCast'] = 1\n",
    "                        actor_sketch['actorType'] = 'cast'\n",
    "                    elif href_url.split('?')[0] == '/Crew/':\n",
    "                        actor_dict['aid'] = href_url.split('?')[1]\n",
    "                        actor_dict['isCast'] = 0\n",
    "                        actor_sketch['actorType'] = 'crew'\n",
    "\n",
    "                else:\n",
    "                    actor_dict['aid'] = actor_dict['name']\n",
    "                    actor_dict['isCast'] = 0\n",
    "                    actor_sketch['actorType'] = actor.css(\"td ::attr(class)\").extract_first()\n",
    "                    if actor_sketch['actorType'] == None:\n",
    "                        actor_sketch['actorType'] = \"unknown\"\n",
    "\n",
    "\n",
    "                if not actor_dict['aid'] in self.actor_seen:\n",
    "                    self.actor_seen.add(actor_dict['aid'])\n",
    "                    yield actor_dict\n",
    "\n",
    "                actor_sketch['tid'] = sketch['tid']\n",
    "                actor_sketch['sid'] = sketch['sid']\n",
    "                actor_sketch['eid'] = sketch['eid']\n",
    "                actor_sketch['aid'] = actor_dict['aid']\n",
    "                actor_sketch['type'] = 'actor_sketch'\n",
    "                \n",
    "                if not actor_sketch['aid'] in actor_seen_title:\n",
    "                    actor_seen_title.add(actor_sketch['aid'])\n",
    "                    yield actor_sketch\n",
    "        yield sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the spider\n",
    "You can also run a spider from a script. To do this you need to import the additional class *CrawlerProcess*. Note that scraping can take a lot of time, so I inserted *break*-statements into *snl_spider*. The code above will only scrape one sketch of one episode of one season. This will be done in a reasonable time. If you want to scrape everything you should call your spider from the commandline using the following command:\n",
    "\n",
    "    scrapy runspider ./snlscrape/spiders/snl.py -o ./data/snl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 01:38:23 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-11-19 01:38:23 [py.warnings] WARNING: /usr/local/lib/python3.11/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-11-19 01:38:23 [scrapy.extensions.telnet] INFO: Telnet Password: b9c6c5ef66d75352\n",
      "2023-11-19 01:38:23 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-11-19 01:38:23 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-11-19 01:38:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-11-19 01:38:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-11-19 01:38:23 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-11-19 01:38:23 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-11-19 01:38:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-11-19 01:38:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6029\n",
      "2023-11-19 01:38:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.snlarchives.net/Seasons/> (referer: None)\n",
      "2023-11-19 01:38:23 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-11-19 01:38:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 232,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2702,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.341186,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 11, 19, 6, 38, 23, 862389, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 17309,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 135544832,\n",
      " 'memusage/startup': 135544832,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 11, 19, 6, 38, 23, 521203, tzinfo=datetime.timezone.utc)}\n",
      "2023-11-19 01:38:23 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<200 http://www.snlarchives.net/Seasons/>\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess, CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "# process = CrawlerProcess({\n",
    "#     # https://docs.scrapy.org/en/latest/topics/feed-exports.html#std-setting-FEEDS\n",
    "#     'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'snl.json'\n",
    "# })\n",
    "\n",
    "# process = CrawlerProcess({\n",
    "#     # https://docs.scrapy.org/en/latest/topics/feed-exports.html#std-setting-FEEDS\n",
    "#     'snl.json': {\n",
    "#         'format': 'json',\n",
    "#         'encoding': 'utf8',\n",
    "#         'item_classes': [snl]\n",
    "#     }\n",
    "# })\n",
    "\n",
    "# runner = CrawlerRunner()\n",
    "# runner.crawl(snl)\n",
    "# deferred = runner.join()\n",
    "# deferred.addBoth(lambda _: reactor.stop())\n",
    "# print('done')\n",
    "\n",
    "\n",
    "\n",
    "# process = CrawlerProcess(\n",
    "#     settings={\n",
    "#         \"FEEDS\": {\n",
    "#             \"items.json\": {\"format\": \"json\"},\n",
    "#         },\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# process.crawl(snl)\n",
    "# process.start()  # the script will block here until the crawling is finished\n",
    "\n",
    "\n",
    "\n",
    "from twisted.internet import reactor\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\"})\n",
    "runner = CrawlerRunner()\n",
    "\n",
    "d = runner.crawl(snl)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()  # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data\n",
    "Once the spider is finished you have a .json-file that contains all of your data. Let's read it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "snl_df = pd.read_json(\"./items.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=0, step=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data frame contains every column from every type of data. This means that the first thing to do is split the data into separate dataframes. To do this I inserted the column *type* to every entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g7/cjd5wdq97cv_5j4_1qlm9qsr0000gn/T/ipykernel_63533/3404710667.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msnl_season\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msnl_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'season'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msnl_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msnl_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'episode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'eid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'aired'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'host'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msnl_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msnl_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'eid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'titleType'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msnl_actor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msnl_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'actor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'isCast'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "snl_season = snl_df[snl_df.type == 'season'][['sid','year']].reset_index(drop=True)\n",
    "snl_episode = snl_df[snl_df.type == 'episode'][['sid','eid','year','aired', 'host']].reset_index(drop=True)\n",
    "snl_title = snl_df[snl_df.type == 'title'][['sid','eid','tid','title','titleType']].reset_index(drop=True)\n",
    "snl_actor = snl_df[snl_df.type == 'actor'][['aid','name','isCast']].reset_index(drop=True)\n",
    "snl_actor_title = snl_df[snl_df.type == 'actor_sketch'][['sid','eid','tid','aid','actorType']].reset_index(drop=True)\n",
    "snl_rating = snl_df[snl_df.type == 'rating'][['sid','eid','1', '10', '2', '3', '4', '5', '6', '7', '8', '9', 'Aged 18-29',\n",
    "       'Aged 18-29_avg', 'Aged 30-44', 'Aged 30-44_avg', 'Aged 45+',\n",
    "       'Aged 45+_avg', 'Aged under 18', 'Aged under 18_avg', 'Females',\n",
    "       'Females Aged 18-29', 'Females Aged 18-29_avg', 'Females Aged 30-44',\n",
    "       'Females Aged 30-44_avg', 'Females Aged 45+', 'Females Aged 45+_avg',\n",
    "       'Females under 18', 'Females under 18_avg', 'Females_avg', 'IMDb staff',\n",
    "       'IMDb staff_avg', 'IMDb users', 'IMDb users_avg', 'Males',\n",
    "       'Males Aged 18-29', 'Males Aged 18-29_avg', 'Males Aged 30-44',\n",
    "       'Males Aged 30-44_avg', 'Males Aged 45+', 'Males Aged 45+_avg',\n",
    "       'Males under 18', 'Males under 18_avg', 'Males_avg', 'Non-US users',\n",
    "       'Non-US users_avg', 'Top 1000 voters', 'Top 1000 voters_avg',\n",
    "       'US users', 'US users_avg']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix some datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_rating['sid'] = snl_rating['sid'].astype(int)\n",
    "snl_rating['eid'] = snl_rating['eid'].astype(int)\n",
    "snl_rating['1'] = snl_rating['1'].fillna(0).astype(int)\n",
    "snl_rating['2'] = snl_rating['2'].fillna(0).astype(int)\n",
    "snl_rating['3'] = snl_rating['3'].fillna(0).astype(int)\n",
    "snl_rating['4'] = snl_rating['4'].fillna(0).astype(int)\n",
    "snl_rating['5'] = snl_rating['5'].fillna(0).astype(int)\n",
    "snl_rating['6'] = snl_rating['6'].fillna(0).astype(int)\n",
    "snl_rating['7'] = snl_rating['7'].fillna(0).astype(int)\n",
    "snl_rating['8'] = snl_rating['8'].fillna(0).astype(int)\n",
    "snl_rating['9'] = snl_rating['9'].fillna(0).astype(int)\n",
    "snl_rating['10'] = snl_rating['10'].fillna(0).astype(int)\n",
    "snl_rating['Aged under 18'] = snl_rating['Aged under 18'].fillna(0).astype(int)\n",
    "snl_rating['Aged 18-29'] = snl_rating['Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Aged 30-44'] = snl_rating['Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Aged 45+'] = snl_rating['Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['Females under 18'] = snl_rating['Females under 18'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 18-29'] = snl_rating['Females Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 30-44'] = snl_rating['Females Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 45+'] = snl_rating['Females Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['Males under 18'] = snl_rating['Males under 18'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 18-29'] = snl_rating['Males Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 30-44'] = snl_rating['Males Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 45+'] = snl_rating['Males Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['IMDb staff'] = snl_rating['IMDb staff'].fillna(0).astype(int)\n",
    "snl_rating['IMDb users'] = snl_rating['IMDb users'].fillna(0).astype(int)\n",
    "snl_rating['US users'] = snl_rating['US users'].fillna(0).astype(int)\n",
    "snl_rating['Non-US users'] = snl_rating['Non-US users'].fillna(0).astype(int)\n",
    "snl_rating['Top 1000 voters'] = snl_rating['Top 1000 voters'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_episode = snl_episode[np.isfinite(snl_episode['eid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_season.sid = pd.to_numeric(snl_season.sid, downcast='integer')\n",
    "snl_season.year = pd.to_numeric(snl_season.year, downcast='integer')\n",
    "\n",
    "snl_episode.sid = pd.to_numeric(snl_episode.sid, downcast='integer')\n",
    "snl_episode.eid = pd.to_numeric(snl_episode.eid, downcast='integer')\n",
    "snl_episode.year = pd.to_numeric(snl_episode.year, downcast='integer')\n",
    "\n",
    "snl_title.sid = pd.to_numeric(snl_title.sid, downcast='integer')\n",
    "snl_title.eid = pd.to_numeric(snl_title.eid, downcast='integer')\n",
    "snl_title.tid = pd.to_numeric(snl_title.tid, downcast='integer')\n",
    "\n",
    "snl_actor.isCast = pd.to_numeric(snl_actor.isCast, downcast='integer')\n",
    "\n",
    "snl_actor_title.sid = pd.to_numeric(snl_actor_title.sid, downcast='integer')\n",
    "snl_actor_title.eid = pd.to_numeric(snl_actor_title.eid, downcast='integer')\n",
    "snl_actor_title.tid = pd.to_numeric(snl_actor_title.tid, downcast='integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 sketches in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_title.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's store the files on the disc so that we can share them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_season.to_csv(\"./db/snl_season.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_episode.to_csv(\"./db/snl_episode.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_title.to_csv(\"./db/snl_title.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_actor.to_csv(\"./db/snl_actor.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_actor_title.to_csv(\"./db/snl_actor_title.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_rating.to_csv(\"./db/snl_rating.csv\", encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
