{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from a public website and IMDb\n",
    "\n",
    "This notebook tracks my progress in scraping the website [snlarchives](http://www.snlarchives.net) for a Saturday Night Live dataset. I will also scrape the ratings for the episodes from IMDb. I chose to use the python library [scrapy](https://scrapy.org/) to do the work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy spiders\n",
    "\n",
    "Scrapy uses so called spiders to crawl the web. This means that I had to create a subclass from scrapy.Spider to begin. Spider *spider123* can be executed by using \n",
    "\n",
    "    scrapy runspider spider123.py \n",
    "\n",
    "in the python command line. This means that the code for each spider needs to be saved into a .py-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import re\n",
    "import string\n",
    "\n",
    "def removeTags(sXML):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  sText = re.sub(cleanr, '', sXML)\n",
    "  return sText\n",
    "\n",
    "class snl(scrapy.Spider):\n",
    "    name = 'snl'\n",
    "    # http://www.snlarchives.net/\n",
    "    start_urls = ['http://www.snlarchives.net/Seasons/']\n",
    "    base_url = \"http://www.snlarchives.net\"\n",
    "    base_url_imdb = \"http://www.imdb.com/title/tt0072562/episodes?season=\"\n",
    "\n",
    "    actor_seen = set()\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    printable = set(string.printable)\n",
    "\n",
    "    def removeTags(self, sXML):\n",
    "        sText = re.sub(self.cleanr, '', sXML)\n",
    "        return sText\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        snl = {}\n",
    "\n",
    "        # parsing snlarchives. Entrypoint is the seasons page at www.snlarchives.net/Seasons/\n",
    "        for season in response.css('div.thumbRectInner'):\n",
    "            sid = int(season.css('::text').extract_first())\n",
    "            year = 1974 + sid\n",
    "            next_page = '?{}'.format(year)\n",
    "\n",
    "            item_season = {}\n",
    "            item_season['sid'] = int(sid)\n",
    "            item_season['year'] = int(year)\n",
    "            item_season['type'] = 'season'\n",
    "\n",
    "            yield item_season\n",
    "\n",
    "            yield scrapy.Request(response.urljoin(next_page), callback=self.parseSeason, meta={'season': item_season})\n",
    "\n",
    "            # at this point we branch out to get the ratings from imdb.com \n",
    "            # the root URL for SNL is http://www.imdb.com/title/tt0072562\n",
    "            # we can find the episodes at http://www.imdb.com/title/tt0072562/episodes\n",
    "            # an we can select a certain episode like this http://www.imdb.com/title/tt0072562/episodes?season=1\n",
    "            yield scrapy.Request(self.base_url_imdb + str(sid), callback=self.parseRatingsSeason, meta={'season': item_season})\n",
    "\n",
    "\n",
    "    def parseRatingsSeason(self, response):\n",
    "        # parsing the ratings of the episodes of a season\n",
    "        item_season = response.meta['season']\n",
    "        eid = 0\n",
    "        for episode in response.css(\".eplist > .list_item > .image > a\"):\n",
    "            item_rating = {}\n",
    "            eid +=1\n",
    "            item_rating[\"type\"] = \"rating\"\n",
    "            item_rating[\"eid\"] = eid\n",
    "            item_rating[\"sid\"] = item_season[\"sid\"]\n",
    "            href_url = episode.css(\"a ::attr(href)\").extract_first()\n",
    "            url_split = href_url.split(\"?\")\n",
    "            href_url = \"http://www.imdb.com\" + url_split[0] + \"ratings\"\n",
    "            yield scrapy.Request(href_url, callback=self.parseRatingsEpisode, meta={'season': item_season, 'rating': item_rating})\n",
    "\n",
    "    def parseRatingsEpisode(self, response):\n",
    "        item_season = response.meta['season']\n",
    "        item_rating = response.meta['rating']\n",
    "        tables = response.css(\"table[cellpadding='0']\")\n",
    "        \n",
    "        # 1st table is vote distribution on rating\n",
    "        # 2nd table is vote distribution on age and gender\n",
    "        ratingTable = tables[0]\n",
    "        ageGenderTable = tables[1]\n",
    "\n",
    "        trCount = 0\n",
    "        for tableTr in ratingTable.css(\"tr\"):\n",
    "            if trCount > 0:\n",
    "                sRating = str(11 - trCount)\n",
    "                item_rating[sRating] = int(removeTags(tableTr.css(\"td\")[0].extract()))\n",
    "            trCount += 1\n",
    "        \n",
    "        trCount = 0\n",
    "        for tableTr in ageGenderTable.css(\"tr\"):\n",
    "            if trCount > 0:\n",
    "                tableTd = tableTr.css(\"td\").extract()\n",
    "                if len(tableTd) > 1:\n",
    "                    sKey = removeTags(tableTd[0]).lstrip().rstrip()\n",
    "                    sValue = int(removeTags(\"\".join(filter(lambda x: x in self.printable, tableTd[1]))))\n",
    "                    sValueAvg = float(removeTags(\"\".join(filter(lambda x: x in self.printable, tableTd[2]))))\n",
    "                    item_rating[sKey] = sValue\n",
    "                    item_rating[sKey + \"_avg\"] = sValueAvg\n",
    "            trCount+=1\n",
    "        yield item_rating\n",
    "\n",
    "    def parseSeason(self, response):\n",
    "        # parsing a season (e.g. www.snlarchives.net/Seasons/?1975)\n",
    "        # episodes is already chosen\n",
    "        item_season = response.meta['season']\n",
    "\n",
    "        for episode in response.css('a'):\n",
    "            href_url = episode.css(\"a ::attr(href)\").extract_first()\n",
    "            if href_url.startswith(\"/Episodes/?\") and len(href_url)==19: \n",
    "                episode_url = self.base_url + href_url\n",
    "                yield scrapy.Request(episode_url, callback=self.parseEpisode, meta={'season': item_season})\n",
    "                # remove statement to scrape more than one episode\n",
    "                #break\n",
    "\n",
    "    def parseEpisode(self, response):\n",
    "        item_season = response.meta['season']\n",
    "\n",
    "        episode = {}\n",
    "        episode['sid'] = item_season['sid']\n",
    "        episode['year'] = item_season['year']\n",
    "        episode['type'] = 'episode'\n",
    "\n",
    "        for epInfoTr in response.css(\"table.epGuests tr\"):\n",
    "            epInfoTd = epInfoTr.css(\"td\")\n",
    "            if epInfoTd[0].css(\"td p ::text\").extract_first() == 'Aired:':\n",
    "                airedInfo = epInfoTd[1].css(\"td p ::text\").extract()\n",
    "                episode['aired'] = airedInfo[0][:-2]\n",
    "                episode['eid'] = int(airedInfo[2].split(' ')[0][1:])\n",
    "            if epInfoTd[0].css(\"td p ::text\").extract_first() == 'Host:':\n",
    "                host = epInfoTd[1].css(\"td p ::text\").extract()\n",
    "                episode['host'] = host[0]\n",
    "            if epInfoTd[0].css(\"td p ::text\").extract_first() == 'Hosts:':\n",
    "                host = epInfoTd[1].css(\"td p ::text\").extract()\n",
    "                episode['host'] = host\n",
    "\n",
    "        yield episode\n",
    "        # initially the titles tab is opened\n",
    "        for sketchInfo in response.css(\"div.sketchWrapper\"):\n",
    "            sketch = {}\n",
    "            href_url = sketchInfo.css(\"a ::attr(href)\").extract_first()\n",
    "            sketch['sid'] = item_season['sid']\n",
    "            sketch['eid'] = episode['eid']\n",
    "            sketch['tid'] = int(href_url.split('?')[1])\n",
    "            sketch['title'] = sketchInfo.css(\".title ::text\").extract_first()\n",
    "            sketch['type'] = 'title'\n",
    "            sketch['titleType'] = sketchInfo.css(\".type ::text\").extract_first()\n",
    "            if sketch['title'] == None:\n",
    "                sketch['title'] = \"\"\n",
    "\n",
    "            sketch_url = self.base_url + href_url\n",
    "            yield scrapy.Request(sketch_url, callback=self.parseTitle, meta={'title': sketch, 'episode': episode})\n",
    "            # remove statement to scrape more than one sketch\n",
    "            #break\n",
    "\n",
    "    def parseTitle(self, response):\n",
    "        sketch = response.meta['title']\n",
    "        episode = response.meta['episode']\n",
    "        actor_seen_title = set()\n",
    "        for actor in response.css(\".roleTable > tr\"):\n",
    "            actor_dict = {}\n",
    "            actor_sketch = {}\n",
    "            actor_dict['name'] = actor.css(\"td ::text\").extract_first()\n",
    "            if actor_dict['name'] == ' ... ':\n",
    "                actor_dict['name'] = episode['host']\n",
    "            if actor_dict['name'] != None:\n",
    "                actor_dict['type'] = 'actor'\n",
    "                href_url = actor.css(\"td > a ::attr(href)\").extract_first()\n",
    "                if href_url != None:\n",
    "                    if href_url.split('?')[0] == '/Cast/':\n",
    "                        actor_dict['aid'] = href_url.split('?')[1]\n",
    "                        actor_dict['isCast'] = 1\n",
    "                        actor_sketch['actorType'] = 'cast'\n",
    "                    elif href_url.split('?')[0] == '/Crew/':\n",
    "                        actor_dict['aid'] = href_url.split('?')[1]\n",
    "                        actor_dict['isCast'] = 0\n",
    "                        actor_sketch['actorType'] = 'crew'\n",
    "\n",
    "                else:\n",
    "                    actor_dict['aid'] = actor_dict['name']\n",
    "                    actor_dict['isCast'] = 0\n",
    "                    actor_sketch['actorType'] = actor.css(\"td ::attr(class)\").extract_first()\n",
    "                    if actor_sketch['actorType'] == None:\n",
    "                        actor_sketch['actorType'] = \"unknown\"\n",
    "\n",
    "\n",
    "                if not actor_dict['aid'] in self.actor_seen:\n",
    "                    self.actor_seen.add(actor_dict['aid'])\n",
    "                    yield actor_dict\n",
    "\n",
    "                actor_sketch['tid'] = sketch['tid']\n",
    "                actor_sketch['sid'] = sketch['sid']\n",
    "                actor_sketch['eid'] = sketch['eid']\n",
    "                actor_sketch['aid'] = actor_dict['aid']\n",
    "                actor_sketch['type'] = 'actor_sketch'\n",
    "                \n",
    "                if not actor_sketch['aid'] in actor_seen_title:\n",
    "                    actor_seen_title.add(actor_sketch['aid'])\n",
    "                    yield actor_sketch\n",
    "        yield sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the spider\n",
    "You can also run a spider from a script. To do this you need to import the additional class *CrawlerProcess*. Note that scraping can take a lot of time, so I inserted *break*-statements into *snl_spider*. The code above will only scrape one sketch of one episode of one season. This will be done in a reasonable time. If you want to scrape everything you should call your spider from the commandline using the following command:\n",
    "\n",
    "    scrapy runspider ./snlscrape/spiders/snl.py -o ./data/snl.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 01:29:50 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: scrapybot)\n",
      "2023-11-19 01:29:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.9.13, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.11.6 (main, Oct  2 2023, 13:45:54) [Clang 15.0.0 (clang-1500.0.40.1)], pyOpenSSL 23.3.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.5, Platform macOS-14.1.1-x86_64-i386-64bit\n",
      "2023-11-19 01:29:50 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-11-19 01:29:50 [py.warnings] WARNING: /usr/local/lib/python3.11/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-11-19 01:29:50 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-11-19 01:29:50 [scrapy.extensions.telnet] INFO: Telnet Password: a59b62414e3efef5\n",
      "2023-11-19 01:29:50 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-11-19 01:29:50 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-11-19 01:29:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-11-19 01:29:50 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-11-19 01:29:50 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-11-19 01:29:50 [scrapy.core.engine] INFO: Spider opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 01:29:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-11-19 01:29:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6029\n",
      "2023-11-19 01:29:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-11-19 01:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.snlarchives.net/Seasons/> (referer: None)\n",
      "2023-11-19 01:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.snlarchives.net/Seasons/> (referer: None)\n",
      "2023-11-19 01:29:51 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-11-19 01:29:51 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: items.json\n",
      "2023-11-19 01:29:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 232,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2702,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.393573,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 11, 19, 6, 29, 51, 58904, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 17309,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 12,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 150323200,\n",
      " 'memusage/startup': 150323200,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 11, 19, 6, 29, 50, 665331, tzinfo=datetime.timezone.utc)}\n",
      "2023-11-19 01:29:51 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2023-11-19 01:29:51 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-11-19 01:29:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 232,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2702,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 102.399835,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 11, 19, 6, 29, 51, 65807, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 17309,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 18,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 150343680,\n",
      " 'memusage/startup': 135241728,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 11, 19, 6, 28, 8, 665972, tzinfo=datetime.timezone.utc)}\n",
      "2023-11-19 01:29:51 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess, CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "\n",
    "# process = CrawlerProcess({\n",
    "#     # https://docs.scrapy.org/en/latest/topics/feed-exports.html#std-setting-FEEDS\n",
    "#     'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'snl.json'\n",
    "# })\n",
    "\n",
    "# process = CrawlerProcess({\n",
    "#     # https://docs.scrapy.org/en/latest/topics/feed-exports.html#std-setting-FEEDS\n",
    "#     'snl.json': {\n",
    "#         'format': 'json',\n",
    "#         'encoding': 'utf8',\n",
    "#         'item_classes': [snl]\n",
    "#     }\n",
    "# })\n",
    "\n",
    "# runner = CrawlerRunner()\n",
    "# runner.crawl(snl)\n",
    "# deferred = runner.join()\n",
    "# deferred.addBoth(lambda _: reactor.stop())\n",
    "# print('done')\n",
    "\n",
    "\n",
    "\n",
    "# process = CrawlerProcess(\n",
    "#     settings={\n",
    "#         \"FEEDS\": {\n",
    "#             \"items.json\": {\"format\": \"json\"},\n",
    "#         },\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# process.crawl(snl)\n",
    "# process.start()  # the script will block here until the crawling is finished\n",
    "\n",
    "\n",
    "\n",
    "from twisted.internet import reactor\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "configure_logging({\"LOG_FORMAT\": \"%(levelname)s: %(message)s\"})\n",
    "runner = CrawlerRunner()\n",
    "\n",
    "d = runner.crawl(snl)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()  # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data\n",
    "Once the spider is finished you have a .json-file that contains all of your data. Let's read it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ./data/snl.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tim/source_code/snldb/snl.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tim/source_code/snldb/snl.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tim/source_code/snldb/snl.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m snl_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(\u001b[39m\"\u001b[39;49m\u001b[39m./data/snl.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/json/_json.py:760\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    758\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[1;32m    761\u001b[0m     path_or_buf,\n\u001b[1;32m    762\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[1;32m    763\u001b[0m     typ\u001b[39m=\u001b[39;49mtyp,\n\u001b[1;32m    764\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    765\u001b[0m     convert_axes\u001b[39m=\u001b[39;49mconvert_axes,\n\u001b[1;32m    766\u001b[0m     convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n\u001b[1;32m    767\u001b[0m     keep_default_dates\u001b[39m=\u001b[39;49mkeep_default_dates,\n\u001b[1;32m    768\u001b[0m     precise_float\u001b[39m=\u001b[39;49mprecise_float,\n\u001b[1;32m    769\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[1;32m    770\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    771\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[1;32m    772\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    773\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    774\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[1;32m    775\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    776\u001b[0m     encoding_errors\u001b[39m=\u001b[39;49mencoding_errors,\n\u001b[1;32m    777\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    778\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[1;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/json/_json.py:861\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    860\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mujson\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 861\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m    862\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/io/json/_json.py:917\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    909\u001b[0m     filepath_or_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n\u001b[1;32m    910\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    911\u001b[0m     \u001b[39misinstance\u001b[39m(filepath_or_buffer, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    912\u001b[0m     \u001b[39mand\u001b[39;00m filepath_or_buffer\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    916\u001b[0m ):\n\u001b[0;32m--> 917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mfilepath_or_buffer\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    919\u001b[0m \u001b[39mreturn\u001b[39;00m filepath_or_buffer\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ./data/snl.json does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "snl_df = pd.read_json(\"./data/snl.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data frame contains every column from every type of data. This means that the first thing to do is split the data into separate dataframes. To do this I inserted the column *type* to every entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_season = snl_df[snl_df.type == 'season'][['sid','year']].reset_index(drop=True)\n",
    "snl_episode = snl_df[snl_df.type == 'episode'][['sid','eid','year','aired', 'host']].reset_index(drop=True)\n",
    "snl_title = snl_df[snl_df.type == 'title'][['sid','eid','tid','title','titleType']].reset_index(drop=True)\n",
    "snl_actor = snl_df[snl_df.type == 'actor'][['aid','name','isCast']].reset_index(drop=True)\n",
    "snl_actor_title = snl_df[snl_df.type == 'actor_sketch'][['sid','eid','tid','aid','actorType']].reset_index(drop=True)\n",
    "snl_rating = snl_df[snl_df.type == 'rating'][['sid','eid','1', '10', '2', '3', '4', '5', '6', '7', '8', '9', 'Aged 18-29',\n",
    "       'Aged 18-29_avg', 'Aged 30-44', 'Aged 30-44_avg', 'Aged 45+',\n",
    "       'Aged 45+_avg', 'Aged under 18', 'Aged under 18_avg', 'Females',\n",
    "       'Females Aged 18-29', 'Females Aged 18-29_avg', 'Females Aged 30-44',\n",
    "       'Females Aged 30-44_avg', 'Females Aged 45+', 'Females Aged 45+_avg',\n",
    "       'Females under 18', 'Females under 18_avg', 'Females_avg', 'IMDb staff',\n",
    "       'IMDb staff_avg', 'IMDb users', 'IMDb users_avg', 'Males',\n",
    "       'Males Aged 18-29', 'Males Aged 18-29_avg', 'Males Aged 30-44',\n",
    "       'Males Aged 30-44_avg', 'Males Aged 45+', 'Males Aged 45+_avg',\n",
    "       'Males under 18', 'Males under 18_avg', 'Males_avg', 'Non-US users',\n",
    "       'Non-US users_avg', 'Top 1000 voters', 'Top 1000 voters_avg',\n",
    "       'US users', 'US users_avg']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix some datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_rating['sid'] = snl_rating['sid'].astype(int)\n",
    "snl_rating['eid'] = snl_rating['eid'].astype(int)\n",
    "snl_rating['1'] = snl_rating['1'].fillna(0).astype(int)\n",
    "snl_rating['2'] = snl_rating['2'].fillna(0).astype(int)\n",
    "snl_rating['3'] = snl_rating['3'].fillna(0).astype(int)\n",
    "snl_rating['4'] = snl_rating['4'].fillna(0).astype(int)\n",
    "snl_rating['5'] = snl_rating['5'].fillna(0).astype(int)\n",
    "snl_rating['6'] = snl_rating['6'].fillna(0).astype(int)\n",
    "snl_rating['7'] = snl_rating['7'].fillna(0).astype(int)\n",
    "snl_rating['8'] = snl_rating['8'].fillna(0).astype(int)\n",
    "snl_rating['9'] = snl_rating['9'].fillna(0).astype(int)\n",
    "snl_rating['10'] = snl_rating['10'].fillna(0).astype(int)\n",
    "snl_rating['Aged under 18'] = snl_rating['Aged under 18'].fillna(0).astype(int)\n",
    "snl_rating['Aged 18-29'] = snl_rating['Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Aged 30-44'] = snl_rating['Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Aged 45+'] = snl_rating['Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['Females under 18'] = snl_rating['Females under 18'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 18-29'] = snl_rating['Females Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 30-44'] = snl_rating['Females Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 45+'] = snl_rating['Females Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['Males under 18'] = snl_rating['Males under 18'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 18-29'] = snl_rating['Males Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 30-44'] = snl_rating['Males Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 45+'] = snl_rating['Males Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['IMDb staff'] = snl_rating['IMDb staff'].fillna(0).astype(int)\n",
    "snl_rating['IMDb users'] = snl_rating['IMDb users'].fillna(0).astype(int)\n",
    "snl_rating['US users'] = snl_rating['US users'].fillna(0).astype(int)\n",
    "snl_rating['Non-US users'] = snl_rating['Non-US users'].fillna(0).astype(int)\n",
    "snl_rating['Top 1000 voters'] = snl_rating['Top 1000 voters'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_episode = snl_episode[np.isfinite(snl_episode['eid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_season.sid = pd.to_numeric(snl_season.sid, downcast='integer')\n",
    "snl_season.year = pd.to_numeric(snl_season.year, downcast='integer')\n",
    "\n",
    "snl_episode.sid = pd.to_numeric(snl_episode.sid, downcast='integer')\n",
    "snl_episode.eid = pd.to_numeric(snl_episode.eid, downcast='integer')\n",
    "snl_episode.year = pd.to_numeric(snl_episode.year, downcast='integer')\n",
    "\n",
    "snl_title.sid = pd.to_numeric(snl_title.sid, downcast='integer')\n",
    "snl_title.eid = pd.to_numeric(snl_title.eid, downcast='integer')\n",
    "snl_title.tid = pd.to_numeric(snl_title.tid, downcast='integer')\n",
    "\n",
    "snl_actor.isCast = pd.to_numeric(snl_actor.isCast, downcast='integer')\n",
    "\n",
    "snl_actor_title.sid = pd.to_numeric(snl_actor_title.sid, downcast='integer')\n",
    "snl_actor_title.eid = pd.to_numeric(snl_actor_title.eid, downcast='integer')\n",
    "snl_actor_title.tid = pd.to_numeric(snl_actor_title.tid, downcast='integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 sketches in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_title.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's store the files on the disc so that we can share them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snl_season.to_csv(\"./db/snl_season.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_episode.to_csv(\"./db/snl_episode.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_title.to_csv(\"./db/snl_title.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_actor.to_csv(\"./db/snl_actor.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_actor_title.to_csv(\"./db/snl_actor_title.csv\", encoding=\"utf-8\", index=False)\n",
    "snl_rating.to_csv(\"./db/snl_rating.csv\", encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
