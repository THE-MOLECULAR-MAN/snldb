{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from a public website\n",
    "\n",
    "This notebook tracks my progress in scraping the website [snlarchives](http://www.snlarchives.net) for a Saturday Night Live dataset. I chose to use the python library [scrapy](https://scrapy.org/) to do the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy spiders\n",
    "\n",
    "Scrapy uses so called spiders to crawl the web. This means that I had to create a subclass from scrapy.Spider to begin. Spider *spider123* can be executed by using \n",
    "\n",
    "    scrapy runspider spider123.py \n",
    "\n",
    "in the python command line. This means that the code for each spider needs to be saved into a .py-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class snl(scrapy.Spider):\n",
    "    name = 'snl'\n",
    "    start_urls = ['http://www.snlarchives.net/Seasons/']\n",
    "    base_url = \"http://www.snlarchives.net\"\n",
    "\n",
    "    actor_seen = set()\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        snl = {}\n",
    "\n",
    "        # parsing snlarchives. Entrypoint is the seasons page at www.snlarchives.net/Seasons/\n",
    "        for season in response.css('div.thumbRectInner'):\n",
    "            sid = int(season.css('::text').extract_first())\n",
    "            year = 1974 + sid\n",
    "            next_page = '?{}'.format(year)\n",
    "\n",
    "            item_season = {}\n",
    "            item_season['sid'] = sid\n",
    "            item_season['year'] = year\n",
    "            item_season['type'] = 'season'\n",
    "\n",
    "            yield item_season\n",
    "\n",
    "            yield scrapy.Request(response.urljoin(next_page), callback=self.parseSeason, meta={'season': item_season})\n",
    "            # remove statement to scrape more than one season\n",
    "            #break\n",
    "            \n",
    "    def parseSeason(self, response):\n",
    "        # parsing a season (e.g. www.snlarchives.net/Seasons/?1975)\n",
    "        # episodes is already chosen\n",
    "        item_season = response.meta['season']\n",
    "\n",
    "        for episode in response.css('a'):\n",
    "            href_url = episode.css(\"a ::attr(href)\").extract_first()\n",
    "            if href_url.startswith(\"/Episodes/?\") and len(href_url)==19: \n",
    "                episode_url = self.base_url + href_url\n",
    "                yield scrapy.Request(episode_url, callback=self.parseEpisode, meta={'season': item_season, 'date': href_url.split('?')[1]})\n",
    "                # remove statement to scrape more than one episode\n",
    "                #break\n",
    "\n",
    "    def parseEpisode(self, response):\n",
    "        item_season = response.meta['season']\n",
    "\n",
    "        episode = {}\n",
    "        episode['date'] = response.meta['date']\n",
    "        episode['sid'] = item_season['sid']\n",
    "        episode['year'] = item_season['year']\n",
    "        episode['type'] = 'episode'\n",
    "\n",
    "        for epInfoTr in response.css(\"table.epGuests tr\"):\n",
    "            epInfoTd = epInfoTr.css(\"td\")\n",
    "            if epInfoTd[0].css(\"td p ::text\").extract_first() == 'Aired:':\n",
    "                episode['aired'] = epInfoTd[1].css(\"td p ::text\").extract()\n",
    "                break\n",
    "\n",
    "        yield episode\n",
    "        # initially the titles tab is opened\n",
    "        for sketchInfo in response.css(\"div.sketchWrapper\"):\n",
    "            sketch = {}\n",
    "            href_url = sketchInfo.css(\"a ::attr(href)\").extract_first()\n",
    "            sketch['sid'] = item_season['sid']\n",
    "            sketch['tid'] = href_url.split('?')[1]\n",
    "            sketch['date'] = episode['date']\n",
    "            sketch['title'] = sketchInfo.css(\".title ::text\").extract_first()\n",
    "            sketch['type'] = 'title'\n",
    "            sketch['titleType'] = sketchInfo.css(\".type ::text\").extract_first()\n",
    "\n",
    "            sketch_url = self.base_url + href_url\n",
    "            yield scrapy.Request(sketch_url, callback=self.parseTitle, meta={'title': sketch})\n",
    "            # remove statement to scrape more than one sketch\n",
    "            #break\n",
    "\n",
    "    def parseTitle(self, response):\n",
    "        sketch = response.meta['title']\n",
    "        for actor in response.css(\".roleTable > tr\"):\n",
    "            actor_dict = {}\n",
    "            actor_sketch = {}\n",
    "            actor_dict['name'] = actor.css(\"td ::text\").extract_first()\n",
    "            actor_dict['type'] = 'actor'\n",
    "            href_url = actor.css(\"td > a ::attr(href)\").extract_first()\n",
    "            if href_url != None and href_url.split('?')[0] == '/Cast/':\n",
    "                actor_dict['aid'] = href_url.split('?')[1]\n",
    "                actor_dict['isCast'] = True\n",
    "                actor_sketch['actorType'] = 'cast'\n",
    "                # possible sraping of the cast members here\n",
    "            else:\n",
    "                actor_dict['aid'] = actor_dict['name']\n",
    "                actor_dict['isCast'] = False\n",
    "                actor_sketch['actorType'] = actor.css(\"td ::attr(class)\").extract_first()\n",
    "            \n",
    "            if not actor_dict['aid'] in self.actor_seen:\n",
    "                self.actor_seen.add(actor_dict['aid'])\n",
    "                yield actor_dict\n",
    "\n",
    "            actor_sketch['tid'] = sketch['tid']\n",
    "            actor_sketch['sid'] = sketch['sid']\n",
    "            actor_sketch['aid'] = actor_dict['aid']\n",
    "            actor_sketch['type'] = 'actor_sketch'\n",
    "            yield actor_sketch\n",
    "        yield sketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the spider\n",
    "You can also run a spider from a script. To do this you need to import the additional class *CrawlerProcess*. Note that scraping can take a lot of time, so I inserted *break*-statements into *snl_spider*. The code above will only scrape one sketch of one episode of one season. This will be done in a reasonable time. If you want to scrape everything you should call your spider from the commandline using the command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 08:14:17 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n",
      "2017-02-08 08:14:17 [scrapy] INFO: Overridden settings: {'FEED_URI': 'snl.json', 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)', 'FEED_FORMAT': 'json'}\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'snl.json'\n",
    "})\n",
    "\n",
    "#process.crawl(snl_spider)\n",
    "#process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data\n",
    "Once the spider is finished you have a .json-file that contains all of your data. Let's read it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_df = pd.read_json(\"./data/snl_v9.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1', '10', '2', '3', '4', '5', '6', '7', '8', '9', 'Aged 18-29',\n",
       "       'Aged 18-29_avg', 'Aged 30-44', 'Aged 30-44_avg', 'Aged 45+',\n",
       "       'Aged 45+_avg', 'Aged under 18', 'Aged under 18_avg', 'Females',\n",
       "       'Females Aged 18-29', 'Females Aged 18-29_avg', 'Females Aged 30-44',\n",
       "       'Females Aged 30-44_avg', 'Females Aged 45+', 'Females Aged 45+_avg',\n",
       "       'Females under 18', 'Females under 18_avg', 'Females_avg', 'IMDb staff',\n",
       "       'IMDb staff_avg', 'IMDb users', 'IMDb users_avg', 'Males',\n",
       "       'Males Aged 18-29', 'Males Aged 18-29_avg', 'Males Aged 30-44',\n",
       "       'Males Aged 30-44_avg', 'Males Aged 45+', 'Males Aged 45+_avg',\n",
       "       'Males under 18', 'Males under 18_avg', 'Males_avg', 'Non-US users',\n",
       "       'Non-US users_avg', 'Top 1000 voters', 'Top 1000 voters_avg',\n",
       "       'US users', 'US users_avg', 'actorType', 'aid', 'aired', 'eid',\n",
       "       'isCast', 'name', 'sid', 'tid', 'title', 'titleType', 'type', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data frame contains every column from every type of data. This means that the first thing to do is split the data into separate dataframes. To do this I inserted the column *type* to every entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_season = snl_df[snl_df.type == 'season'][['sid','year']].reset_index()\n",
    "snl_episode = snl_df[snl_df.type == 'episode'][['sid','eid','year','aired']].reset_index()\n",
    "snl_title = snl_df[snl_df.type == 'title'][['sid','eid','tid','title','titleType']].reset_index()\n",
    "snl_actor = snl_df[snl_df.type == 'actor'][['aid','name','isCast']].reset_index()\n",
    "snl_actor_sketch = snl_df[snl_df.type == 'actor_sketch'][['sid','eid','tid','aid','actorType']].reset_index()\n",
    "snl_rating = snl_df[snl_df.type == 'rating'][['sid','eid','1', '10', '2', '3', '4', '5', '6', '7', '8', '9', 'Aged 18-29',\n",
    "       'Aged 18-29_avg', 'Aged 30-44', 'Aged 30-44_avg', 'Aged 45+',\n",
    "       'Aged 45+_avg', 'Aged under 18', 'Aged under 18_avg', 'Females',\n",
    "       'Females Aged 18-29', 'Females Aged 18-29_avg', 'Females Aged 30-44',\n",
    "       'Females Aged 30-44_avg', 'Females Aged 45+', 'Females Aged 45+_avg',\n",
    "       'Females under 18', 'Females under 18_avg', 'Females_avg', 'IMDb staff',\n",
    "       'IMDb staff_avg', 'IMDb users', 'IMDb users_avg', 'Males',\n",
    "       'Males Aged 18-29', 'Males Aged 18-29_avg', 'Males Aged 30-44',\n",
    "       'Males Aged 30-44_avg', 'Males Aged 45+', 'Males Aged 45+_avg',\n",
    "       'Males under 18', 'Males under 18_avg', 'Males_avg', 'Non-US users',\n",
    "       'Non-US users_avg', 'Top 1000 voters', 'Top 1000 voters_avg',\n",
    "       'US users', 'US users_avg']].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix some datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_rating['sid'] = snl_rating['sid'].astype(int)\n",
    "snl_rating['eid'] = snl_rating['eid'].astype(int)\n",
    "snl_rating['1'] = snl_rating['1'].fillna(0).astype(int)\n",
    "snl_rating['2'] = snl_rating['2'].fillna(0).astype(int)\n",
    "snl_rating['3'] = snl_rating['3'].fillna(0).astype(int)\n",
    "snl_rating['4'] = snl_rating['4'].fillna(0).astype(int)\n",
    "snl_rating['5'] = snl_rating['5'].fillna(0).astype(int)\n",
    "snl_rating['6'] = snl_rating['6'].fillna(0).astype(int)\n",
    "snl_rating['7'] = snl_rating['7'].fillna(0).astype(int)\n",
    "snl_rating['8'] = snl_rating['8'].fillna(0).astype(int)\n",
    "snl_rating['9'] = snl_rating['9'].fillna(0).astype(int)\n",
    "snl_rating['10'] = snl_rating['10'].fillna(0).astype(int)\n",
    "snl_rating['Aged under 18'] = snl_rating['Aged under 18'].fillna(0).astype(int)\n",
    "snl_rating['Aged 18-29'] = snl_rating['Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Aged 30-44'] = snl_rating['Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Aged 45+'] = snl_rating['Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['Females under 18'] = snl_rating['Females under 18'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 18-29'] = snl_rating['Females Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 30-44'] = snl_rating['Females Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Females Aged 45+'] = snl_rating['Females Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['Males under 18'] = snl_rating['Males under 18'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 18-29'] = snl_rating['Males Aged 18-29'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 30-44'] = snl_rating['Males Aged 30-44'].fillna(0).astype(int)\n",
    "snl_rating['Males Aged 45+'] = snl_rating['Males Aged 45+'].fillna(0).astype(int)\n",
    "snl_rating['IMDb staff'] = snl_rating['IMDb staff'].fillna(0).astype(int)\n",
    "snl_rating['IMDb users'] = snl_rating['IMDb users'].fillna(0).astype(int)\n",
    "snl_rating['US users'] = snl_rating['US users'].fillna(0).astype(int)\n",
    "snl_rating['Non-US users'] = snl_rating['Non-US users'].fillna(0).astype(int)\n",
    "snl_rating['Top 1000 voters'] = snl_rating['Top 1000 voters'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_episode = snl_episode[np.isfinite(snl_episode['eid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_season.sid = pd.to_numeric(snl_season.sid)\n",
    "snl_season.year = pd.to_numeric(snl_season.year)\n",
    "\n",
    "snl_episode.sid = pd.to_numeric(snl_episode.sid)\n",
    "snl_episode.eid = pd.to_numeric(snl_episode.eid)\n",
    "snl_episode.year = pd.to_numeric(snl_episode.year)\n",
    "\n",
    "snl_title.sid = pd.to_numeric(snl_title.sid)\n",
    "snl_title.eid = pd.to_numeric(snl_title.eid)\n",
    "snl_title.tid = pd.to_numeric(snl_title.tid)\n",
    "\n",
    "snl_actor.isCast = pd.to_numeric(snl_actor.isCast)\n",
    "\n",
    "snl_actor_sketch.sid = pd.to_numeric(snl_actor_sketch.sid)\n",
    "snl_actor_sketch.eid = pd.to_numeric(snl_actor_sketch.eid)\n",
    "snl_actor_sketch.tid = pd.to_numeric(snl_actor_sketch.tid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 sketches in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>eid</th>\n",
       "      <th>tid</th>\n",
       "      <th>title</th>\n",
       "      <th>titleType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>197701221</td>\n",
       "      <td>Injured John</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>197805201</td>\n",
       "      <td>Nixon's Book</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>197805131</td>\n",
       "      <td>Paraquat</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>198101241</td>\n",
       "      <td>America Not Held Hostage Anymore</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>198104111</td>\n",
       "      <td>Storage Room</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>198202271</td>\n",
       "      <td>CBS Evening News</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>197905261</td>\n",
       "      <td>Mr. Bill</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>197804221</td>\n",
       "      <td>Rock Concert</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>198205151</td>\n",
       "      <td>Kaufman Fight</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>198205221</td>\n",
       "      <td>Women's Room</td>\n",
       "      <td>Cold Opening</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sid  eid        tid                             title     titleType\n",
       "82     2   12  197701221                      Injured John  Cold Opening\n",
       "91     3   20  197805201                      Nixon's Book  Cold Opening\n",
       "99     3   19  197805131                          Paraquat  Cold Opening\n",
       "113    6    8  198101241  America Not Held Hostage Anymore  Cold Opening\n",
       "118    6   13  198104111                      Storage Room  Cold Opening\n",
       "121    7   13  198202271                  CBS Evening News  Cold Opening\n",
       "135    4   20  197905261                          Mr. Bill  Cold Opening\n",
       "151    3   18  197804221                      Rock Concert  Cold Opening\n",
       "153    7   19  198205151                     Kaufman Fight  Cold Opening\n",
       "158    7   20  198205221                      Women's Room  Cold Opening"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snl_title.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's store the files on the disc so that we can share them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snl_season.to_csv(\"./db/snl_season.csv\")\n",
    "snl_episode.to_csv(\"./db/snl_episode.csv\")\n",
    "snl_title.to_csv(\"./db/snl_title.csv\")\n",
    "snl_actor.to_csv(\"./db/snl_actor.csv\")\n",
    "snl_actor_sketch.to_csv(\"./db/snl_actor_sketch.csv\")\n",
    "snl_ratings.to_csv(\"./db/snl_ratings.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
